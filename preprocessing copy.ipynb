{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "9d59a9a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "sys.path.insert(0, '..')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "d7090b8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import torchaudio\n",
    "from torchaudio.transforms import MelSpectrogram\n",
    "\n",
    "from params import sample_rate, windowed_signal_length, num_mel_bands, overlap, hop_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "be58823d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MelSpecPipeline(torch.nn.Module):\n",
    "    def __init__(self, n_fft=windowed_signal_length, sample_rate=sample_rate, n_mel=num_mel_bands):\n",
    "        super().__init__()\n",
    "        self.mel_spec = MelSpectrogram(sample_rate=sample_rate, n_fft=n_fft, n_mels=n_mel, power=2, center=False, hop_length=hop_length)\n",
    "\n",
    "    def forward(self, wave):\n",
    "        assert wave.shape[0] == 1\n",
    "\n",
    "        mel_spec = self.mel_spec(wave)\n",
    "        return mel_spec\n",
    "    \n",
    "pipeline = MelSpecPipeline()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e3d0f66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{(124.63, 136.975), (72.876, 82.036), (194.072, 205.542), (158.155, 162.49), (0.582, 16.477), (233.761, 246.866), (14.198, 25.438), (262.443, 270.543), (90.981, 106.051), (273.033, 288.833), (52.268, 68.272), (137.211, 151.031), (250.985, 265.71), (273.62, 287.805), (123.585, 134.32), (98.198, 112.692), (207.596, 223.196), (208.436, 220.976), (234.433, 249.023)}\n",
      "Speech times ratio of speech vs non-speech: 1.830188679245283\n",
      "Shape of mels: torch.Size([40, 18685])\n",
      "mel_slice_start: 18640, mel_slice_end: 18680, mel_time_start: 298.24, mel_time_end: 298.88\n",
      "ratio of speech to non-speech labels: 170.43%\n"
     ]
    }
   ],
   "source": [
    "def check_audio_metadata(metadata):\n",
    "    assert metadata.sample_rate == 16000\n",
    "    assert metadata.num_channels == 1\n",
    "    assert metadata.num_frames > 0\n",
    "\n",
    "def speechOverlap(mel_time_start, mel_time_end, speech_segments):\n",
    "    for speech_start, speech_end in speech_segments:\n",
    "        if speech_start < mel_time_end and mel_time_start < speech_end:\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "def createDataFromRecording(session_root, id):\n",
    "    wav_path = session_root + \"/session_\" + str(id) + \"_mixture.wav\"\n",
    "    json_path = session_root + \"/session_\" + str(id) + \".json\"\n",
    "\n",
    "    # check some metadata\n",
    "    metadata = torchaudio.info(wav_path)\n",
    "    check_audio_metadata(metadata)\n",
    "    # print(f'Metadata: {metadata}')\n",
    "\n",
    "    # retrieve speech segments\n",
    "    speech_segments = set()\n",
    "    with open(json_path, 'r') as f:\n",
    "        speech_info = json.load(f)\n",
    "    for key in speech_info:\n",
    "        if key.isdigit():\n",
    "            for info in speech_info[key]:\n",
    "                segment = (info[\"start\"], info[\"stop\"])\n",
    "                assert segment[0] < segment[1]\n",
    "                speech_segments.add(segment)\n",
    "    print(speech_segments)\n",
    "    speech_times = [int(speechOverlap(i, i + 1, speech_segments)) for i in range(300)]\n",
    "    print(f'Speech times ratio of speech vs non-speech: {sum(speech_times) / (len(speech_times) - sum(speech_times))}')\n",
    "\n",
    "    # MFSC pipeline\n",
    "    wave, _ = torchaudio.load(wav_path)\n",
    "    mels = pipeline(wave)\n",
    "    mels.squeeze_(0)\n",
    "    # librosa.display.specshow(mels.numpy())\n",
    "    print(f'Shape of mels: {mels.shape}')\n",
    "    num_data = mels.shape[1] // (num_mel_bands // overlap)\n",
    "\n",
    "    one_mel_length_time = (windowed_signal_length // overlap) / sample_rate\n",
    "\n",
    "    X = []\n",
    "    y = []\n",
    "\n",
    "    for i in range(num_data - 1):\n",
    "        mel_slice_start = i * (num_mel_bands // overlap)\n",
    "        mel_slice_end = mel_slice_start + num_mel_bands\n",
    "\n",
    "        mel_time_start = mel_slice_start * one_mel_length_time\n",
    "        mel_time_end = mel_slice_end * one_mel_length_time\n",
    "\n",
    "        X.append(mels[:, mel_slice_start : mel_slice_end].clone().detach())\n",
    "        y.append(torch.ones(1) if speechOverlap(mel_time_start, mel_time_end, speech_segments) else torch.zeros(1))\n",
    "\n",
    "        if i == num_data - 2:\n",
    "            # print the 4 mel values above\n",
    "            print(f'mel_slice_start: {mel_slice_start}, mel_slice_end: {mel_slice_end}, mel_time_start: {mel_time_start}, mel_time_end: {mel_time_end}')\n",
    "\n",
    "    X = torch.cat(X, dim=0)\n",
    "    y = torch.cat(y, dim=0)\n",
    "    return X, y\n",
    "\n",
    "X, y = createDataFromRecording(session_root=\"LibriParty/dataset/train/session_0\", id=0)\n",
    "print(f'ratio of speech to non-speech labels: {(((y==1).sum() / (y==0).sum()) * 100):.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "3e7385b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def createDataset(root, num_sessions):\n",
    "#     X = []\n",
    "#     y = []\n",
    "#     for session in range(num_sessions):\n",
    "#         session_X, session_y = createDataFromRecording(session_root=root + \"session_\" + str(session), id=session)\n",
    "#         assert torch.is_tensor(session_X) and torch.is_tensor(session_y) \n",
    "#         assert session_X.shape[1] == num_mel_bands and session_X.shape[1] == num_mel_bands and session_y.shape[1] == 1 and session_X.shape[0] == session_y.shape[0]\n",
    "\n",
    "#         X.append(session_X)\n",
    "#         y.append(session_y)\n",
    "#     X = torch.cat(X, dim=0)\n",
    "#     y = torch.cat(y, dim=0)\n",
    "\n",
    "#     print(f'X.shape is: {X.shape}')\n",
    "#     print(f'y.shape is: {y.shape}')\n",
    "#     return X, y\n",
    "\n",
    "# X, y = createDataset(root='LibriParty/dataset/dev/', num_sessions=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "01d24a41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num 0s: 345\n",
      "num 1s: 588\n"
     ]
    }
   ],
   "source": [
    "print(f'num 0s: {(y==0).sum()}')\n",
    "print(f'num 1s: {(y==1).sum()}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DSP_ML",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
