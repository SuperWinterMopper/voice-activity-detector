{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "9d59a9a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "sys.path.insert(0, '..')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "d7090b8a",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'overlap' from 'params' (c:\\Users\\afkhe\\Programming\\Projects\\Machine Learning\\voice-activity-detector\\params.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[58], line 22\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorchaudio\u001b[39;00m\n\u001b[0;32m     20\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorchaudio\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtransforms\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m MelSpectrogram, AmplitudeToDB, TimeMasking, FrequencyMasking\n\u001b[1;32m---> 22\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mparams\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m sample_rate, windowed_signal_length, num_mel_bands, overlap\n",
      "\u001b[1;31mImportError\u001b[0m: cannot import name 'overlap' from 'params' (c:\\Users\\afkhe\\Programming\\Projects\\Machine Learning\\voice-activity-detector\\params.py)"
     ]
    }
   ],
   "source": [
    "from __future__ import annotations\n",
    "import os\n",
    "import math\n",
    "import argparse\n",
    "from dataclasses import dataclass\n",
    "from typing import List, Tuple, Dict\n",
    "import json\n",
    "\n",
    "import numpy as np\n",
    "from scipy import signal\n",
    "from scipy.fft import fft, fftshift\n",
    "import matplotlib.pyplot as plt\n",
    "import librosa\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchaudio\n",
    "from torchaudio.transforms import MelSpectrogram, AmplitudeToDB, TimeMasking, FrequencyMasking\n",
    "\n",
    "from params import sample_rate, windowed_signal_length, num_mel_bands, overlap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be58823d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MelSpecPipeline(torch.nn.Module):\n",
    "    def __init__(self, n_fft=windowed_signal_length, sample_rate=sample_rate, n_mel=num_mel_bands):\n",
    "        super().__init__()\n",
    "        self.mel_spec = MelSpectrogram(sample_rate=sample_rate, n_fft=n_fft, n_mels=n_mel, power=2)\n",
    "\n",
    "    def forward(self, wave):\n",
    "        assert wave.shape[0] == 1\n",
    "\n",
    "        mel_spec = self.mel_spec(wave)\n",
    "        return mel_spec\n",
    "    \n",
    "pipeline = MelSpecPipeline()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e3d0f66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metadata: AudioMetaData(sample_rate=16000, num_frames=4783692, num_channels=1, bits_per_sample=32, encoding=PCM_F)\n",
      "{(124.63, 136.975), (72.876, 82.036), (194.072, 205.542), (158.155, 162.49), (0.582, 16.477), (233.761, 246.866), (14.198, 25.438), (262.443, 270.543), (90.981, 106.051), (273.033, 288.833), (52.268, 68.272), (137.211, 151.031), (250.985, 265.71), (273.62, 287.805), (123.585, 134.32), (98.198, 112.692), (207.596, 223.196), (208.436, 220.976), (234.433, 249.023)}\n",
      "Shape of mels: torch.Size([40, 18687])\n",
      "mel_length_time: 1.28\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'overlap' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[51], line 58\u001b[0m\n\u001b[0;32m     54\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mX[\u001b[39m\u001b[38;5;132;01m{\u001b[39;00moutput_i\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m].shape: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mX[output_i]\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     56\u001b[0m         \u001b[38;5;66;03m# assert X[output_i].shape == (num_mel_bands, num_mel_bands)\u001b[39;00m\n\u001b[1;32m---> 58\u001b[0m \u001b[43mcreateDataFromRecording\u001b[49m\u001b[43m(\u001b[49m\u001b[43msession_root\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mLibriParty/dataset/train/session_0\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mid\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[51], line 37\u001b[0m, in \u001b[0;36mcreateDataFromRecording\u001b[1;34m(session_root, id)\u001b[0m\n\u001b[0;32m     34\u001b[0m mel_length_time \u001b[38;5;241m=\u001b[39m (windowed_signal_length \u001b[38;5;241m*\u001b[39m num_mel_bands) \u001b[38;5;241m/\u001b[39m sample_rate\n\u001b[0;32m     35\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmel_length_time: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmel_length_time\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m---> 37\u001b[0m num_data \u001b[38;5;241m=\u001b[39m mels\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m num_mel_bands \u001b[38;5;241m*\u001b[39m \u001b[43moverlap\u001b[49m\n\u001b[0;32m     38\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnum_data: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_data\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     40\u001b[0m X \u001b[38;5;241m=\u001b[39m [torch\u001b[38;5;241m.\u001b[39mones(num_mel_bands, num_mel_bands)] \u001b[38;5;241m*\u001b[39m num_data\n",
      "\u001b[1;31mNameError\u001b[0m: name 'overlap' is not defined"
     ]
    }
   ],
   "source": [
    "def check_audio_metadata(metadata):\n",
    "    assert metadata.sample_rate == 16000\n",
    "    assert metadata.num_channels == 1\n",
    "    assert metadata.num_frames > 0\n",
    "\n",
    "def createDataFromRecording(session_root, id):\n",
    "    wav_path = session_root + \"/session_\" + str(id) + \"_mixture.wav\"\n",
    "    json_path = session_root + \"/session_\" + str(id) + \".json\"\n",
    "\n",
    "    # check some metadata\n",
    "    metadata = torchaudio.info(wav_path)\n",
    "    check_audio_metadata(metadata)\n",
    "    print(f'Metadata: {metadata}')\n",
    "\n",
    "    # retrieve speech segments\n",
    "    speech_segments = set()\n",
    "    with open(json_path, 'r') as f:\n",
    "        speech_info = json.load(f)\n",
    "    for key in speech_info:\n",
    "        if key.isdigit():\n",
    "            for info in speech_info[key]:\n",
    "                segment = (info[\"start\"], info[\"stop\"])\n",
    "                assert segment[0] < segment[1]\n",
    "                speech_segments.add(segment)\n",
    "    print(speech_segments)\n",
    "\n",
    "    # MFSC pipeline\n",
    "    wave, _ = torchaudio.load(wav_path)\n",
    "    mels = pipeline(wave)\n",
    "    mels.squeeze_(0)\n",
    "    # librosa.display.specshow(mels.numpy())\n",
    "    print(f'Shape of mels: {mels.shape}')\n",
    "\n",
    "    mel_length_time = (windowed_signal_length * num_mel_bands) / sample_rate\n",
    "    print(f'mel_length_time: {mel_length_time}')\n",
    "\n",
    "    num_data = mels.shape[1] // num_mel_bands * overlap\n",
    "    print(f'num_data: {num_data}')\n",
    "\n",
    "    X = [torch.ones(num_mel_bands, num_mel_bands)] * num_data\n",
    "    y = [0.0] * num_data\n",
    "\n",
    "    # we iterate over all complete mel bands possible. \n",
    "    # We drop the end bit of the recording if it isn't long enough for a whole MFSC spectrogram\n",
    "    # We process with overlaps of 1/`overlap`\n",
    "    for output_i, mel_i in enumerate(range(0, int(mels.shape[1] - num_mel_bands), num_mel_bands // overlap)):\n",
    "        mel_start_time = mel_i * mel_length_time\n",
    "        mel_end_time = (mel_i + 1) * mel_length_time\n",
    "\n",
    "        for start, end in speech_segments:\n",
    "            if start < mel_start_time < end or start < mel_end_time < end:\n",
    "                y[output_i] = 1.0\n",
    "        X[output_i] = mels[:, mel_i : mel_i + num_mel_bands]\n",
    "        print(f'X[{output_i}].shape: {X[output_i].shape}')\n",
    "\n",
    "        # assert X[output_i].shape == (num_mel_bands, num_mel_bands)\n",
    "    \n",
    "createDataFromRecording(session_root=\"LibriParty/dataset/train/session_0\", id=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e7385b9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DSP_ML",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
