{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9d59a9a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "sys.path.insert(0, '..')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d7090b8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from json import JSONDecodeError\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import torchaudio\n",
    "from torchaudio.transforms import MelSpectrogram\n",
    "\n",
    "from params import sample_rate, windowed_signal_length, num_mel_bands, overlap, hop_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "be58823d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MelSpecPipeline(torch.nn.Module):\n",
    "    def __init__(self, n_fft=windowed_signal_length, sample_rate=sample_rate, n_mel=num_mel_bands):\n",
    "        super().__init__()\n",
    "        self.mel_spec = MelSpectrogram(sample_rate=sample_rate, n_fft=n_fft, n_mels=n_mel, power=2, center=False, hop_length=hop_length)\n",
    "\n",
    "    def forward(self, wave):\n",
    "        assert wave.shape[0] == 1\n",
    "\n",
    "        mel_spec = self.mel_spec(wave)\n",
    "        return mel_spec\n",
    "    \n",
    "pipeline = MelSpecPipeline()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0e3d0f66",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_audio_metadata(metadata):\n",
    "    assert metadata.sample_rate == 16000\n",
    "    assert metadata.num_channels == 1\n",
    "    assert metadata.num_frames > 0\n",
    "\n",
    "def speechOverlap(mel_time_start, mel_time_end, speech_segments):\n",
    "    for speech_start, speech_end in speech_segments:\n",
    "        if speech_start < mel_time_end and mel_time_start < speech_end:\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "def createDataFromRecording(session_root, id):\n",
    "    wav_path = session_root + \"/session_\" + str(id) + \"_mixture.wav\"\n",
    "    json_path = session_root + \"/session_\" + str(id) + \".json\"\n",
    "\n",
    "    # check some metadata\n",
    "    metadata = torchaudio.info(wav_path)\n",
    "    check_audio_metadata(metadata)\n",
    "\n",
    "    wave_len = (metadata.num_frames // metadata.sample_rate)\n",
    "    # print(f'wave_len: {wave_len}')\n",
    "    # print(f'Metadata: {metadata}')\n",
    "\n",
    "    # retrieve speech segments\n",
    "    speech_segments = set()\n",
    "    \n",
    "    # read+parse with diagnostics so we know which file fails\n",
    "    with open(json_path, 'rb') as f:\n",
    "        _raw = f.read()\n",
    "    try:\n",
    "        speech_info = json.loads(_raw.decode('utf-8'))\n",
    "    except JSONDecodeError as _e:\n",
    "        # provide filename, size and a short preview to help debugging\n",
    "        print(f'Error decoding JSON at file: {json_path}')\n",
    "        _preview = repr(_raw[:400])\n",
    "        raise RuntimeError(f\"Failed to parse JSON file {json_path!r} (size={len(_raw)} bytes). preview={_preview}\") from _e\n",
    "    for key in speech_info:\n",
    "        if key.isdigit():\n",
    "            for info in speech_info[key]:\n",
    "                segment = (info[\"start\"], info[\"stop\"])\n",
    "                assert segment[0] < segment[1]\n",
    "                speech_segments.add(segment)\n",
    "        \n",
    "    # print(speech_segments)\n",
    "\n",
    "    # MFSC pipeline\n",
    "    wave, _ = torchaudio.load(wav_path)\n",
    "    mels = pipeline(wave)\n",
    "    mels.squeeze_(0)\n",
    "    # librosa.display.specshow(mels.numpy())\n",
    "    # print(f'Shape of mels: {mels.shape}')\n",
    "\n",
    "    num_data = mels.shape[1] // (num_mel_bands // overlap)\n",
    "    # print(f'NUM DATA: {num_data}')\n",
    "\n",
    "    one_mel_length_time = (windowed_signal_length // overlap) / sample_rate\n",
    "\n",
    "    X = []\n",
    "    y = []\n",
    "\n",
    "    for i in range(num_data - 1):\n",
    "        mel_slice_start = i * (num_mel_bands // overlap)\n",
    "        mel_slice_end = mel_slice_start + num_mel_bands\n",
    "\n",
    "        mel_time_start = mel_slice_start * one_mel_length_time\n",
    "        mel_time_end = mel_slice_end * one_mel_length_time\n",
    "\n",
    "        X.append(mels[:, mel_slice_start : mel_slice_end].clone().detach())\n",
    "        y.append(torch.ones(1) if speechOverlap(mel_time_start, mel_time_end, speech_segments) else torch.zeros(1))\n",
    "\n",
    "        # if i == num_data // 1.02:\n",
    "        #     print(f'mel_slice_start: {mel_slice_start}, mel_slice_end: {mel_slice_end}, mel_time_start: {mel_time_start}, mel_time_end: {mel_time_end}')\n",
    "        #     print(f'y[-1] is {y[-1]}')\n",
    "\n",
    "        assert X[-1].shape == (num_mel_bands, num_mel_bands)\n",
    "        assert y[-1].shape == (1,)\n",
    "\n",
    "    X = torch.stack(X, dim=0)\n",
    "    y = torch.stack(y, dim=0)\n",
    "\n",
    "    # sanity check ensuring our speech time amount is more or less accurate\n",
    "    times_to_check = np.arange(start=0, stop=wave_len, step=wave_len / num_data)\n",
    "    speech_times = [int(speechOverlap(times_to_check[i], times_to_check[i + 1], speech_segments)) for i in range(len(times_to_check) - 1)]\n",
    "\n",
    "    speech_ratio_theory  = sum(speech_times) / len(speech_times)\n",
    "    speech_ratio_data = ((y == 1).sum() / len(y))\n",
    "    required_closeness_percentage = .05\n",
    "    if abs(speech_ratio_theory - speech_ratio_data) > required_closeness_percentage:\n",
    "        print(f'theoretical ratio of speech lables: {speech_ratio_theory}')\n",
    "        print(f\"This data's ratio of speech labels: {speech_ratio_data}\")\n",
    "        assert False\n",
    "\n",
    "    assert all(isinstance(x, torch.Tensor) for x in X), \"all X entries must be tensors\"\n",
    "    assert all(isinstance(t, torch.Tensor) for t in y), \"all y entries must be tensors\"\n",
    "\n",
    "    return X, y\n",
    "\n",
    "X, y = createDataFromRecording(session_root=\"LibriParty/dataset/train/session_0\", id=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3e7385b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def createDataset(root, num_sessions):\n",
    "    X = []\n",
    "    y = []\n",
    "    for session in range(num_sessions):\n",
    "        session_X, session_y = createDataFromRecording(session_root=root + \"session_\" + str(session), id=session)\n",
    "        assert torch.is_tensor(session_X) and torch.is_tensor(session_y) \n",
    "        assert session_X.shape[1] == num_mel_bands and session_X.shape[1] == num_mel_bands and session_y.shape[1] == 1 and session_X.shape[0] == session_y.shape[0]\n",
    "\n",
    "        X.append(session_X)\n",
    "        y.append(session_y)\n",
    "    X = torch.cat(X, dim=0)\n",
    "    y = torch.cat(y, dim=0)\n",
    "\n",
    "    if X.dim() == 3:\n",
    "        X = X.unsqueeze(1)\n",
    "    y.squeeze_()\n",
    "    X = X.to(torch.float32)\n",
    "    y = y.to(torch.float32)\n",
    "\n",
    "    print(f'X.shape is: {X.shape}')\n",
    "    print(f'y.shape is: {y.shape}')\n",
    "\n",
    "    assert X.shape[1] == 1 and X.shape[2] == num_mel_bands and X.shape[3] == num_mel_bands\n",
    "    assert y.dim() == 1\n",
    "\n",
    "    return TensorDataset(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d7d01fad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X.shape is: torch.Size([45461, 1, 40, 40])\n",
      "y.shape is: torch.Size([45461])\n"
     ]
    }
   ],
   "source": [
    "valid_ds = createDataset(root='LibriParty/dataset/dev/', num_sessions=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3f9917eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X.shape is: torch.Size([45068, 1, 40, 40])\n",
      "y.shape is: torch.Size([45068])\n"
     ]
    }
   ],
   "source": [
    "test_ds = createDataset(root='LibriParty/dataset/eval/', num_sessions=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "01d24a41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X.shape is: torch.Size([226699, 1, 40, 40])\n",
      "y.shape is: torch.Size([226699])\n"
     ]
    }
   ],
   "source": [
    "train_ds = createDataset(root='LibriParty/dataset/train/', num_sessions=250)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9a23c00a",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(train_ds, \"train_ds.pt\")\n",
    "torch.save(valid_ds, \"valid_ds.pt\")\n",
    "torch.save(test_ds, \"test_ds.pt\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DSP_ML",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
